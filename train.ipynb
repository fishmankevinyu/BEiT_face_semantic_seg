{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitFeatureExtractor, BeitForSemanticSegmentation, BeitConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "from pathlib import Path, PurePosixPath, PureWindowsPath\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# load image + ground truth map\n",
    "\n",
    "path = 'headsegmentation_dataset_ccncsa/training.xml'\n",
    "image_folder = 'headsegmentation_dataset_ccncsa'\n",
    "\n",
    "def create_dataset(path, image_folder):\n",
    "    mytree = ET.parse(path)\n",
    "    myroot = mytree.getroot()\n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "    data_size = 100\n",
    "    for child in myroot:\n",
    "        try:\n",
    "            file_name = PureWindowsPath(child.attrib[\"name\"])\n",
    "            #file_name = PurePosixPath(file_name)\n",
    "            image_path = image_folder /file_name\n",
    "            image = cv2.imread(str(image_path), cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            image = np.array(image)\n",
    "            seg_map = np.zeros((IMG_HEIGHT, IMG_WIDTH))\n",
    "            if child.tag == 'srcimg':\n",
    "                img_data_array.append(image)\n",
    "            elif child.tag == 'labelimg':    \n",
    "                for i in range(IMG_HEIGHT):\n",
    "                    for j in range(IMG_WIDTH):\n",
    "                        if np.array_equal(image[i,j],[0,0,0]):\n",
    "                            seg_map[i,j] = 0\n",
    "                        elif np.array_equal(image[i,j],[255,0,0]):\n",
    "                            seg_map[i,j] = 1\n",
    "                        elif np.array_equal(image[i,j],[0,255,0]):\n",
    "                            seg_map[i,j] = 2\n",
    "                        elif np.array_equal(image[i,j],[0,0,255]):\n",
    "                            seg_map[i,j] = 3\n",
    "                        elif np.array_equal(image[i,j],[128,128,128]):\n",
    "                            seg_map[i,j] = 4\n",
    "                        elif np.array_equal(image[i,j],[255,255,0]):\n",
    "                            seg_map[i,j] = 5\n",
    "                        elif np.array_equal(image[i,j],[255,0,255]):\n",
    "                            seg_map[i,j] = 6\n",
    "                        elif np.array_equal(image[i,j],[0,255,255]):\n",
    "                            seg_map[i,j] = 7\n",
    "                        elif np.array_equal(image[i,j],[255,255,255]):\n",
    "                            seg_map[i,j] = 8\n",
    "                        elif np.array_equal(image[i,j],[255,192,192]):\n",
    "                            seg_map[i,j] = 9\n",
    "                        elif np.array_equal(image[i,j],[0,128,128]):\n",
    "                            seg_map[i,j] = 10                \n",
    "                class_name.append(seg_map)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if len(class_name) == data_size:\n",
    "            break\n",
    "            \n",
    "    return img_data_array, class_name\n",
    "\n",
    "X, y = create_dataset(path, image_folder)\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [4. 4. 4. ... 0. 0. 0.]\n",
      " [4. 4. 4. ... 0. 0. 0.]\n",
      " [4. 4. 4. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGhCAYAAADbf0s2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqNklEQVR4nO3df3DTdZ7H8VeANvy4NmepbZIl9KpTxtMyIMUFWZWiUqwCKt4J4ty1tx6jJzDTKYwr6zh073ao6464O3K6ezssBYWDuxlA72AWy0KLDMcMFtgF3GOrlqUszfVksKHIpgW+94fX2LTpj7T58UnyfMx8x+b7/ST5fL/EvPL+fD/5xmZZliUAAAw0It4dAACgL4QUAMBYhBQAwFiEFADAWIQUAMBYhBQAwFiEFADAWIQUAMBYhBQAwFiEFADAWHENqbffflv5+fkaPXq0ioqK9NFHH8WzOwAAw8QtpHbs2KGKigq98sorOnHihO6//36Vlpbq/Pnz8eoSAMAwtnhdYHbGjBmaNm2a3nnnncC6v/zLv9QTTzyh6urqfu978+ZNXbx4URkZGbLZbNHuKgAgwizL0pUrV+R2uzViRN/10qgY9imgo6NDDQ0Nevnll4PWl5SU6MiRI73a+/1++f3+wO0//vGPuvPOO6PeTwBAdDU3N2vChAl9bo9LSH3xxRe6ceOGcnNzg9bn5ubK6/X2al9dXa0f/OAHvdbfp0c1SmlR6ycAIDquq1OHtVcZGRn9totLSHXpOVRnWVbI4bs1a9aosrIycNvn88nj8WiU0jTKRkgBQML5/xNNA52yiUtIZWdna+TIkb2qptbW1l7VlSTZ7XbZ7fZYdQ8AYIi4zO5LT09XUVGRamtrg9bX1tZq1qxZ8egSAMBAcRvuq6ys1N/8zd9o+vTpuvfee/Uv//IvOn/+vF544YV4dQkAYJi4hdTixYt16dIl/eM//qNaWlpUWFiovXv3Ki8vL15dAgAYJm7fkxoOn88nh8OhYj3OxAkASEDXrU7V6X21tbUpMzOzz3Zcuw8AYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGAsQgoAYCxCCgBgLEIKAGCsiIdUdXW17rnnHmVkZCgnJ0dPPPGEzp49G9SmvLxcNpstaJk5c2akuwIASHARD6n6+notX75cR48eVW1tra5fv66SkhJdvXo1qN0jjzyilpaWwLJ3795IdwUAkOBGRfoBf/WrXwXd3rRpk3JyctTQ0KAHHnggsN5ut8vpdEb66QEASSTq56Ta2tokSVlZWUHr6+rqlJOTo0mTJmnZsmVqbW3t8zH8fr98Pl/QAgBIflENKcuyVFlZqfvuu0+FhYWB9aWlpdq6dasOHDigN954Q8eOHdODDz4ov98f8nGqq6vlcDgCi8fjiWa3AQCGsFmWZUXrwZcvX649e/bo8OHDmjBhQp/tWlpalJeXp+3bt2vRokW9tvv9/qAA8/l88ng8KtbjGmVLi0rfAQDRc93qVJ3eV1tbmzIzM/tsF/FzUl1WrlypDz74QIcOHeo3oCTJ5XIpLy9PjY2NIbfb7XbZ7fZodBMAYLCIh5RlWVq5cqV27dqluro65efnD3ifS5cuqbm5WS6XK9LdAQAksIifk1q+fLnee+89bdu2TRkZGfJ6vfJ6vbp27Zokqb29XatXr9Z//dd/6dy5c6qrq9OCBQuUnZ2tJ598MtLdAQAksIhXUu+8844kqbi4OGj9pk2bVF5erpEjR+rUqVPasmWLvvzyS7lcLs2ZM0c7duxQRkZGpLsDAEhgURnu68+YMWO0b9++SD8tACAJce0+AICxCCkAgLEIKQCAsQgpAICxCCkAgLEIKQCAsQgpAICxCCkAgLEIKQCAsaJ2FXQgEe27eDIijzPPPTUijwOkOiopAICxqKSQ9AaqjqJR9XQ9JxUVMDxUUgAAY1FJIeVF6jzUcB6bigsIjZBCwunvjZ83eyC5MNwHADAWlRTiIlpDbNEcuoumUP2mKgSopAAABqOSQtQlanUDIP4IKUQUgRQ5fNcKYLgPAGAwKikMGVVTbOy7eJJqCimLSgoAYCwqKQwJVVRs9TzeVFZIFYQUwkI4AYglhvsAAMaiksKgUEGZpfu/B0N/SGZUUgAAYxFSGBBVFIB4YbgPIRFMiYOhPyQzKikAgLEiHlJVVVWy2WxBi9PpDGy3LEtVVVVyu90aM2aMiouLdebMmUh3A0hJ+y6epApGUolKJXXXXXeppaUlsJw6dSqw7fXXX9f69eu1YcMGHTt2TE6nU3PnztWVK1ei0RUAQAKLyjmpUaNGBVVPXSzL0k9+8hO98sorWrRokSRp8+bNys3N1bZt2/T888+HfDy/3y+/3x+47fP5otFtAIBholJJNTY2yu12Kz8/X0uWLNHnn38uSWpqapLX61VJSUmgrd1u1+zZs3XkyJE+H6+6uloOhyOweDyeaHQbYrgIgFkiHlIzZszQli1btG/fPv3iF7+Q1+vVrFmzdOnSJXm9XklSbm5u0H1yc3MD20JZs2aN2traAktzc3Okuw0AMFDEh/tKS0sDf0+ePFn33nuvbr/9dm3evFkzZ86UJNlstqD7WJbVa113drtddrs90l1NeclUMYWaet1YU9RrXUF5Q6/tA60LxfRjxw8mIllEfQr6uHHjNHnyZDU2NgbOU/WsmlpbW3tVVwAARP3LvH6/X7/73e90//33Kz8/X06nU7W1tbr77rslSR0dHaqvr9ePfvSjaHcFSaZ7lRCqagolVLvBruteXXU9dyJUVFRTSGQRD6nVq1drwYIFmjhxolpbW/XDH/5QPp9PZWVlstlsqqio0Lp161RQUKCCggKtW7dOY8eO1dKlSyPdFfTB9DfW/vR8wx1sOEVC9+fqCqxECSsgUUU8pC5cuKBnnnlGX3zxhW699VbNnDlTR48eVV5eniTppZde0rVr1/Tiiy/q8uXLmjFjhj788ENlZGREuisAgARnsyzLincnwuXz+eRwOFSsxzXKlhbv7iSURPvEP9gJEfHS1wQLk44zw30w0XWrU3V6X21tbcrMzOyzHReYRVwM5o2zsaZIqol6V4alr8Cc5+69Ll7BxUw/JDIuMAsAMBaVFOLKpKG7aGOSBRA+KikAgLEIKcRFY01RSlVRqbSvQCQx3IeYCTpxXxOvXsRGz8srDXSZJQChUUkBAIxFJYWo66qgUnHIK9Q+d01PZwIFMDAqKQCAsaikEFXz3FNTsoIajO7n6KiqgNAIKUQFVzcID9+hAkJjuA8AYCwqqRQRi0/opl8M1hSD/fVfAFRSAACDUUlhyKichqe/H1GUOD8FSIRU0ovlGx0BBSDSGO4DABiLSiqJRbOKYop5ZHFlCiA0KikAgLEIqSQWq2on1X52I9aoWpHKGO7DoDCTL/a6f5+KK1IgVVFJAQCMRSWFsFA9xV5jTVFErk6x7+JJhg6RcKikAADGopLCoFBBAYgHQgpIAN98SDgZz24AMcdwHwDAWIRUkpvnnjrsk+WcbDfHcP49+XdEIiKkAADGIqRSRCQqKgCItYiH1F/8xV/IZrP1WpYvXy5JKi8v77Vt5syZke4GACAJRHx237Fjx3Tjxo3A7dOnT2vu3Ln667/+68C6Rx55RJs2bQrcTk9Pj3Q3AABJIOIhdeuttwbdfu2113T77bdr9uzZgXV2u11Op3PQj+n3++X3+wO3fT7f8DsKJLB57qlcxw8pIarnpDo6OvTee+/pu9/9rmw2W2B9XV2dcnJyNGnSJC1btkytra39Pk51dbUcDkdg8Xg80ew2AMAQNsuyrGg9+L/9279p6dKlOn/+vNzur3/BbceOHfqzP/sz5eXlqampSa+++qquX7+uhoYG2e32kI8TqpLyeDwq1uMaZUuLVveTUjifvrsmWnC1CfMUlDcMqZJi8gxMcd3qVJ3eV1tbmzIzM/tsF9UrTmzcuFGlpaWBgJKkxYsXB/4uLCzU9OnTlZeXpz179mjRokUhH8dut/cZYACA5BW1kPrDH/6g/fv3a+fOnf22c7lcysvLU2NjY7S6AgBIUFELqU2bNiknJ0ePPfZYv+0uXbqk5uZmuVyuaHUFiIlQP6fBUCkwPFEJqZs3b2rTpk0qKyvTqFHfPEV7e7uqqqr01FNPyeVy6dy5c/r+97+v7OxsPfnkk9HoCgzS/U08md+8n/3vC4G/t97xzfqe+9zXb0RF89h0ncfi3BQSRVRCav/+/Tp//ry++93vBq0fOXKkTp06pS1btujLL7+Uy+XSnDlztGPHDmVkZESjKwCABBaVkCopKVGoSYNjxozRvn37ovGUiKKC8oZhfbrvqhi6VxjShdCNw7T1jgmSzKjMgvcv1Loe2/879ON0VV+xqKi6o7qCibh2HwDAWPzoYZKL91UJCsobQlYYyairqhvu/nbdv+po/KtDIN4IqSTXNYQT67AKNSmg601cGt4beffHMdFw+tf9uHQdQxOGMoF4YbgPAGAsKin0aSjTlXtWUM/+94VelUVjTVHQ1OyhMq3CiMTkkq13TEiZ4VFgMKikAADGopJKYiach6o6+rhU07utaVVQvHU/Hl1VZldFtfWO6B0vpp3DdIQUBrTv4knd9mF4b5KE0NB1HbvuQ6JMokCqYrgPAGAsKilEFJ/0E8u+iycZ8oPRqKQAAMYipJJYrD8hU0VFVmNNEccUKY/hviQW70siITIIKqQyKikAgLEIKSDF7bt4kqobxiKkAADGIqSSGFOLASQ6Jk5AUuhAYwgIQLxRSQEAjEUllcSGUgml1bkkSZ3FLTH5wcRQF6UdCFOyvxbpf5+h/DQLEG1UUgAAY1FJIaS0Opc6i1sk/f8n65rIPXao6sm3dOag7pu57aixVwQPtyocTv8ba4qGVIUCiYaQQsx0f1MdbCj15Fs6U5nbjgYez5SgCmffuve/S7j7QUAhVTDcBwAwFpUUgnQN8XVNoIiGoVZRPe8fz6G/4QxZdm/XVVUBCI1KCgBgLCopSOp/+nGkqpRvHqdzWJVI9/bxqkSGWw0OFxMnkCoIKQwoGhMUej5eQXlDVANnsG/og93PgfraX4gxxAcMHsN9AABjUUkhSNew3/zfu4InT5yL7vNGY+JD9+ppxNQ7w75Pl1B966+/A1WFkdhXhvqQKsKupA4dOqQFCxbI7XbLZrNp9+7dQdsty1JVVZXcbrfGjBmj4uJinTlzJqiN3+/XypUrlZ2drXHjxmnhwoW6cOHCsHYEAJB8wq6krl69qilTpujv/u7v9NRTT/Xa/vrrr2v9+vWqqanRpEmT9MMf/lBz587V2bNnlZGRIUmqqKjQf/zHf2j79u0aP368Vq1apfnz56uhoUEjR44c/l4B3Qy2igrV9ubJT8J+vlhNh+cq9UgFYYdUaWmpSktLQ26zLEs/+clP9Morr2jRokWSpM2bNys3N1fbtm3T888/r7a2Nm3cuFHvvvuuHn74YUnSe++9J4/Ho/3792vevHnD2B3gm6GwcMKpLyOm3tlraM2Uq1wAqSCiEyeamprk9XpVUlISWGe32zV79mwdOXJEktTQ0KDOzs6gNm63W4WFhYE2Pfn9fvl8vqAFAJD8IhpSXq9XkpSbmxu0Pjc3N7DN6/UqPT1dt9xyS59teqqurpbD4QgsHo8nkt0G+jVi6p0RqcoAhC8qU9BtNlvQbcuyeq3rqb82a9asUVtbW2Bpbm6OWF+RXGIx6y3ZZ9ZxrgsmiWhIOZ1OSepVEbW2tgaqK6fTqY6ODl2+fLnPNj3Z7XZlZmYGLQCA5BfRkMrPz5fT6VRtbW1gXUdHh+rr6zVr1ixJUlFRkdLS0oLatLS06PTp04E2wHBEa3iOYT8g9sKe3dfe3q5PP/00cLupqUknT55UVlaWJk6cqIqKCq1bt04FBQUqKCjQunXrNHbsWC1dulSS5HA49Nxzz2nVqlUaP368srKytHr1ak2ePDkw2w8AAGkIIfXxxx9rzpw5gduVlZWSpLKyMtXU1Oill17StWvX9OKLL+ry5cuaMWOGPvzww8B3pCTpzTff1KhRo/T000/r2rVreuihh1RTU8N3pDBkkZx2PpBUqKb6u+AwEEs2y7KseHciXD6fTw6HQ8V6XKNsafHujrGGcwJ8/u+Dvwv3yTn3MHsTXbEMKembL/nG6ztT3SdvRHOiAyGFaLludapO76utra3feQZcYBYAYCwuMJuEUnEKcbyG4OL1y8Cx+j0phv0Qb1RSAABjUUkhoQWqiRhXUl2V21AuQAtg8KikkLBMuPJD13enTOgLkIwIKQCAsRjuQ0idxS3Bv8xrkKH84m60da+m+CkPIHKopAAAxiKkkLBMqaJSQSp+rQFmIKSQcEy/0CuTKIDIIaQAAMZi4gQSSkF5Q8y/EzVY3au7aE+iiEe1xtUnEA9UUgAAY1FJoU+dxS2SZOxU9FQXr8kMVFSIJSophNT9DbArrAAg1ggpAICxGO5LIpEe/un+eLd9yFUUBouLzgKRQyUFADAWlRQGhevSDax7BZVMU8+BeKKSQkJprCnSzZOfGD2kFosg5zJFSBWEFADAWIRUkkjFT9YmV1MAIoOQAgAYi4kTSDhd53wKyhsC1VS8rooei8kSpupevXP1CUQLIYWENVAo9DcTbsTUOwc9XNj/88Q2mFJxWBepjeE+AICxqKSQtEJVQF3VVSoP0wGJhEoKAGAsKimkFKomILEQUgmOE+kAklnYw32HDh3SggUL5Ha7ZbPZtHv37sC2zs5Ofe9739PkyZM1btw4ud1u/e3f/q0uXrwY9BjFxcWy2WxBy5IlS4a9MwCA5BJ2SF29elVTpkzRhg0bem376quvdPz4cb366qs6fvy4du7cqd///vdauHBhr7bLli1TS0tLYPn5z38+tD1ATBWUN3CR0zgw/bjvu3iSqh5REfZwX2lpqUpLS0Nuczgcqq2tDVr31ltv6dvf/rbOnz+viRMnBtaPHTtWTqcz3KfH/4v1G4LpPxk+lDfwnuenIvEYACIr6rP72traZLPZ9Od//udB67du3ars7GzdddddWr16ta5cudLnY/j9fvl8vqAFAJD8ojpx4k9/+pNefvllLV26VJmZmYH1zz77rPLz8+V0OnX69GmtWbNGv/nNb3pVYV2qq6v1gx/8IJpdRQIKVfk0vzorjPsfGfL9Pf90pFcfqKqAyItaSHV2dmrJkiW6efOm3n777aBty5YtC/xdWFiogoICTZ8+XcePH9e0adN6PdaaNWtUWVkZuO3z+eTxeKLVdQCAIaISUp2dnXr66afV1NSkAwcOBFVRoUybNk1paWlqbGwMGVJ2u112uz0aXUWC6Vk9hVM59RTp+3avzCJZVZk8YaKnfRdPGnveEokp4iHVFVCNjY06ePCgxo8fP+B9zpw5o87OTrlcrkh3JymZMIuqoLwhJsNbkQylaOvet9Gfff3frmFBaXjBZcK/ORAPYYdUe3u7Pv3008DtpqYmnTx5UllZWXK73fqrv/orHT9+XP/5n/+pGzduyOv1SpKysrKUnp6uzz77TFu3btWjjz6q7OxsffLJJ1q1apXuvvtufec734ncngEAEl7YIfXxxx9rzpw5gdtd54rKyspUVVWlDz74QJI0derUoPsdPHhQxcXFSk9P169//Wv99Kc/VXt7uzwejx577DGtXbtWI0eOHMaupI557qlx+2Qdy6noBeUNRldOg9HV/+4V1WAl0jAfEC1hh1RxcbEsy+pze3/bJMnj8ai+vj7cpwUApCCugg4YjvNRSGVcYDYBmfCmte/iSd32Id8LGshQhvkAfINKCgBgLCopDFnXiX2utNBbVwWVatPO+Y4UIo1KCgBgLCopDFu0KqrGmqJe19dLhCnpwz0PFZh6frH/dkAqIKQwZLH4zlRX8HW9cXv+6YixQRWJq0vw3SggGMN9AABjEVKImGhWAY01RQkzQWO4/eRXboFvEFIAAGNxTgrD1v1Tf7S/4BtqMoUUvwkVkbrKeVcVmkgVFNPNEQuEVAJJpDewaOoeBvGaaNBzBl+iDEUCiYbhPgCAsaikEgAVVN++maL+dWUT62G/4VZQiTjM14Vf4UUsUEkBAIxFSBkuET9hx1Msrjru+acjEZkSzxd3gYEx3GeoRA2neF10Nvj5/EF96W6ww4Ghwq7rOSIZTon67wzECpUUAMBYVFJIWqGmqoc7HBjNipAqChgYlRQAwFhUUgbiE3bkmfBl20Sebg7ECyFlEN68ACAYw30AAGNRSRmACgqJiKtNIBaopAAAxiKkAADGYrgPEWXCLDoAyYNKCgBgLEIKAGAsQsoA89xTk2KmVDLsAwCzhB1Shw4d0oIFC+R2u2Wz2bR79+6g7eXl5bLZbEHLzJkzg9r4/X6tXLlS2dnZGjdunBYuXKgLFy4Ma0cAAMkn7JC6evWqpkyZog0bNvTZ5pFHHlFLS0tg2bt3b9D2iooK7dq1S9u3b9fhw4fV3t6u+fPn68aNG+HvAQAgaYU9u6+0tFSlpaX9trHb7XI6nSG3tbW1aePGjXr33Xf18MMPS5Lee+89eTwe7d+/X/PmzQu3SwCAJBWVc1J1dXXKycnRpEmTtGzZMrW2tga2NTQ0qLOzUyUlJYF1brdbhYWFOnIk9M8o+P1++Xy+oAVAfCTLOVQkhoiHVGlpqbZu3aoDBw7ojTfe0LFjx/Tggw/K7//611K9Xq/S09N1yy23BN0vNzdXXq835GNWV1fL4XAEFo/HE+luAwAMFPEv8y5evDjwd2FhoaZPn668vDzt2bNHixYt6vN+lmXJZrOF3LZmzRpVVlYGbvt8PoIKAFJA1K844XK5lJeXp8bGRkmS0+lUR0eHLl++HFRNtba2atasWSEfw263y263R7urGKKuoR+uNpHcGOJDPET9e1KXLl1Sc3OzXC6XJKmoqEhpaWmqra0NtGlpadHp06f7DCkAQGoKu5Jqb2/Xp59+Grjd1NSkkydPKisrS1lZWaqqqtJTTz0ll8ulc+fO6fvf/76ys7P15JNPSpIcDoeee+45rVq1SuPHj1dWVpZWr16tyZMnB2b7ATALVRTiJeyQ+vjjjzVnzpzA7a5zRWVlZXrnnXd06tQpbdmyRV9++aVcLpfmzJmjHTt2KCMjI3CfN998U6NGjdLTTz+ta9eu6aGHHlJNTY1GjhwZgV1KXF1vBIny+1K8cQGItrBDqri4WJZl9bl93759Az7G6NGj9dZbb+mtt94K9+kBACmEn+ow0Dz31ISppiQmTACIHi4wCwAwFpUUECPfVJwn49kNIKEQUkASC2dyS6ghZibHIN4Y7gMAGItKylCJMB2dCRNDE81/21CVT9e/U0F5Q9j3BeKNSgoAYCwqqQQwmE+4sa645rmnSjUxfUoMoPvrhCoXyYKQMtxAARXrGWMMCZlnsBf4jXRwDTR8CEQCw30AAGNRSSWQ/j4Jx/oqFQwnIdRrgOoKkUYlBQAwFpWUQUyuTjgXZR4Tf2yS6gqRRkjFWDTfUGIxC9CkN8REFYlh2UT60NBYU0RQYcgY7gMAGItKKgJMqC4G24fbPhzccEzPT/sm7GOyGM4VJxL1u1D99ZUqC/2hkgIAGItKaggS6RPsYITan3nuHitqYtIVDFIyvQaZbIH+EFIDSKY3g3Ck6n6bIBEuLhxtTLZAF4b7AADGopIKgSoC8ZJIU8ujbbA/MYLklpIhRQghnrpef93P+wVekzXfrOu+PdWH/iTCKlUx3AcAMFZKVlKACQb70xoF5Q0MA4rJFKmKSgoAYCxCCjBc94qrsaaIc6pIKQz3AQmAYPoakyhSD5UUAMBYSV9J8QkUSD5UVKmDSgoAYCxCCkDCYqQk+YUdUocOHdKCBQvkdrtls9m0e/fuoO02my3k8uMf/zjQpri4uNf2JUuWDHtnumMWFJAa+H89uYUdUlevXtWUKVO0YcOGkNtbWlqCll/+8pey2Wx66qmngtotW7YsqN3Pf/7zoe0BACBphT1xorS0VKWlpX1udzqdQbfff/99zZkzR7fddlvQ+rFjx/Zq2xe/3y+/3x+47fP5+m3Ppyog9TCZIjlF9ZzU//zP/2jPnj167rnnem3bunWrsrOzddddd2n16tW6cuVKn49TXV0th8MRWDweTzS7DQAwRFSnoG/evFkZGRlatGhR0Ppnn31W+fn5cjqdOn36tNasWaPf/OY3qq2tDfk4a9asUWVlZeC2z+cLCioqJwBduMZfcolqSP3yl7/Us88+q9GjRwetX7ZsWeDvwsJCFRQUaPr06Tp+/LimTZvW63HsdrvsdnvI5yCgAPTE0F/yiNpw30cffaSzZ8/q7//+7wdsO23aNKWlpamxsTFa3QEAJKCohdTGjRtVVFSkKVOmDNj2zJkz6uzslMvlCus5PvvZ1CH2DkAqYHp64gt7uK+9vV2ffvpp4HZTU5NOnjyprKwsTZw4UdLX54z+/d//XW+88Uav+3/22WfaunWrHn30UWVnZ+uTTz7RqlWrdPfdd+s73/nOMHYFAJBswg6pjz/+WHPmzAnc7prQUFZWppqaGknS9u3bZVmWnnnmmV73T09P169//Wv99Kc/VXt7uzwejx577DGtXbtWI0eOHOJuAACSkc2yLCvenQiXz+f7eir6z9ZqxJjRA98BQMpjEoVZrludqtP7amtrU2ZmZp/tuHYfAMBYhBSAlMAkisRESAEAjEVIAUgpVFSJhZACABiLkAIAGIuQApCSGPJLDIQUAMBYhBSAlMUkCvMRUgBSHkFlLkIKAGAsQgoAYCxCCgBgLEIKAMQkClMRUgDQDWFlFkIKAGAsQgoAYCxCCgBCYMjPDIQUAMBYhBQAwFiEFADAWIQUAPSB6ejxR0gBAIxFSAEAjEVIAcAAGPaLH0IKAGAsQgoABolqKvYIKQCAsQgpAICxCCkAgLHCCqnq6mrdc889ysjIUE5Ojp544gmdPXs2qI1lWaqqqpLb7daYMWNUXFysM2fOBLXx+/1auXKlsrOzNW7cOC1cuFAXLlwY/t4AAJJKWCFVX1+v5cuX6+jRo6qtrdX169dVUlKiq1evBtq8/vrrWr9+vTZs2KBjx47J6XRq7ty5unLlSqBNRUWFdu3ape3bt+vw4cNqb2/X/PnzdePGjcjtGQBEAdPRY8tmWZY11Dv/7//+r3JyclRfX68HHnhAlmXJ7XaroqJC3/ve9yR9XTXl5ubqRz/6kZ5//nm1tbXp1ltv1bvvvqvFixdLki5evCiPx6O9e/dq3rx5Az6vz+eTw+GQ52drNWLM6KF2HwCGraC8Id5dSEjXrU7V6X21tbUpMzOzz3bDOifV1tYmScrKypIkNTU1yev1qqSkJNDGbrdr9uzZOnLkiCSpoaFBnZ2dQW3cbrcKCwsDbXry+/3y+XxBCwAg+Q05pCzLUmVlpe677z4VFhZKkrxeryQpNzc3qG1ubm5gm9frVXp6um655ZY+2/RUXV0th8MRWDwez1C7DQARxdBfdA05pFasWKHf/va3+td//dde22w2W9Bty7J6reupvzZr1qxRW1tbYGlubh5qtwEACWRIIbVy5Up98MEHOnjwoCZMmBBY73Q6JalXRdTa2hqorpxOpzo6OnT58uU+2/Rkt9uVmZkZtAAAkl9YIWVZllasWKGdO3fqwIEDys/PD9qen58vp9Op2trawLqOjg7V19dr1qxZkqSioiKlpaUFtWlpadHp06cDbQAAkKRR4TRevny5tm3bpvfff18ZGRmBisnhcGjMmDGy2WyqqKjQunXrVFBQoIKCAq1bt05jx47V0qVLA22fe+45rVq1SuPHj1dWVpZWr16tyZMn6+GHH478HgIAElZYIfXOO+9IkoqLi4PWb9q0SeXl5ZKkl156SdeuXdOLL76oy5cva8aMGfrwww+VkZERaP/mm29q1KhRevrpp3Xt2jU99NBDqqmp0ciRI4e3NwAQB12TJ5iOHnnD+p5UvPA9KQAmIqQGLybfkwIAfIOrUUQeIQUAMBYhBQAwFiEFABHGkF/kEFIAAGMRUgAAYxFSABAFzPSLDEIKAGAsQgoAYCxCCgBgLEIKAKKIc1PDQ0gBQAwQVENDSAEAjEVIAQCMRUgBAIxFSAEAjEVIAUCMMNMvfIQUAMBYhBQAxBgV1eARUgAQJwTVwAgpAICxCCkAgLEIKQCAsQgpAICxCCkAiCNm+vWPkAIAGIuQAgADUFGFRkgBAIxFSAEAjEVIAYBBGPILNireHRgKy7IkSTev+ePcEwCIvOtWZ7y7EHXX9fU+dr2f98VmDdTCQBcuXJDH44l3NwAAw9Tc3KwJEyb0uT0hQ+rmzZs6e/as7rzzTjU3NyszMzPeXUpoPp9PHo+HYzlMHMfI4VhGhsnH0bIsXblyRW63WyNG9H3mKSGH+0aMGKFvfetbkqTMzEzjDn6i4lhGBscxcjiWkWHqcXQ4HAO2YeIEAMBYhBQAwFgJG1J2u11r166V3W6Pd1cSHscyMjiOkcOxjIxkOI4JOXECAJAaEraSAgAkP0IKAGAsQgoAYCxCCgBgLEIKAGCshA2pt99+W/n5+Ro9erSKior00UcfxbtLRquqqpLNZgtanE5nYLtlWaqqqpLb7daYMWNUXFysM2fOxLHHZjh06JAWLFggt9stm82m3bt3B20fzHHz+/1auXKlsrOzNW7cOC1cuFAXLlyI4V6YYaBjWV5e3us1OnPmzKA2HEupurpa99xzjzIyMpSTk6MnnnhCZ8+eDWqTTK/LhAypHTt2qKKiQq+88opOnDih+++/X6WlpTp//ny8u2a0u+66Sy0tLYHl1KlTgW2vv/661q9frw0bNujYsWNyOp2aO3eurly5Escex9/Vq1c1ZcoUbdiwIeT2wRy3iooK7dq1S9u3b9fhw4fV3t6u+fPn68aNG7HaDSMMdCwl6ZFHHgl6je7duzdoO8dSqq+v1/Lly3X06FHV1tbq+vXrKikp0dWrVwNtkup1aSWgb3/729YLL7wQtO6OO+6wXn755Tj1yHxr1661pkyZEnLbzZs3LafTab322muBdX/6058sh8Nh/exnP4tRD80nydq1a1fg9mCO25dffmmlpaVZ27dvD7T54x//aI0YMcL61a9+FbO+m6bnsbQsyyorK7Mef/zxPu/DsQyttbXVkmTV19dblpV8r8uEq6Q6OjrU0NCgkpKSoPUlJSU6cuRInHqVGBobG+V2u5Wfn68lS5bo888/lyQ1NTXJ6/UGHVO73a7Zs2dzTPsxmOPW0NCgzs7OoDZut1uFhYUc2xDq6uqUk5OjSZMmadmyZWptbQ1s41iG1tbWJknKysqSlHyvy4QLqS+++EI3btxQbm5u0Prc3Fx5vd449cp8M2bM0JYtW7Rv3z794he/kNfr1axZs3Tp0qXAceOYhmcwx83r9So9PV233HJLn23wtdLSUm3dulUHDhzQG2+8oWPHjunBBx+U3//1j5tyLHuzLEuVlZW67777VFhYKCn5XpcJ+VMdkmSz2YJuW5bVax2+UVpaGvh78uTJuvfee3X77bdr8+bNgZPTHNOhGcpx49j2tnjx4sDfhYWFmj59uvLy8rRnzx4tWrSoz/ul8rFcsWKFfvvb3+rw4cO9tiXL6zLhKqns7GyNHDmyV9q3trb2+uSAvo0bN06TJ09WY2NjYJYfxzQ8gzluTqdTHR0dunz5cp9tEJrL5VJeXp4aGxslcSx7WrlypT744AMdPHgw6Jdtk+11mXAhlZ6erqKiItXW1gatr62t1axZs+LUq8Tj9/v1u9/9Ti6XS/n5+XI6nUHHtKOjQ/X19RzTfgzmuBUVFSktLS2oTUtLi06fPs2xHcClS5fU3Nwsl8sliWPZxbIsrVixQjt37tSBAweUn58ftD3pXpdxm7IxDNu3b7fS0tKsjRs3Wp988olVUVFhjRs3zjp37ly8u2asVatWWXV1ddbnn39uHT161Jo/f76VkZEROGavvfaa5XA4rJ07d1qnTp2ynnnmGcvlclk+ny/OPY+vK1euWCdOnLBOnDhhSbLWr19vnThxwvrDH/5gWdbgjtsLL7xgTZgwwdq/f791/Phx68EHH7SmTJliXb9+PV67FRf9HcsrV65Yq1atso4cOWI1NTVZBw8etO69917rW9/6Fseyh3/4h3+wHA6HVVdXZ7W0tASWr776KtAmmV6XCRlSlmVZ//zP/2zl5eVZ6enp1rRp0wLTLxHa4sWLLZfLZaWlpVlut9tatGiRdebMmcD2mzdvWmvXrrWcTqdlt9utBx54wDp16lQce2yGgwcPWpJ6LWVlZZZlDe64Xbt2zVqxYoWVlZVljRkzxpo/f751/vz5OOxNfPV3LL/66iurpKTEuvXWW620tDRr4sSJVllZWa/jxLG0Qh5DSdamTZsCbZLpdcnvSQEAjJVw56QAAKmDkAIAGIuQAgAYi5ACABiLkAIAGIuQAgAYi5ACABiLkAIAGIuQAgAYi5ACABiLkAIAGOv/AFV8gYHUG1wSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y[0], interpolation='nearest')\n",
    "print(y[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeitFeatureExtractor {\n",
      "  \"crop_size\": 224,\n",
      "  \"do_center_crop\": false,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"reduce_labels\": false,\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7)\n",
    "model_name = \"microsoft/beit-base-finetuned-ade-640-640\"\n",
    "feature_extractor = BeitFeatureExtractor.from_pretrained(model_name, size=224 )\n",
    "print(feature_extractor)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, feature_extractor):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        encoded_inputs = self.feature_extractor(self.images[idx], self.labels[idx], return_tensors=\"pt\")\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_set = CustomDataset(X_train, y_train, feature_extractor)\n",
    "val_set = CustomDataset(X_test, y_test, feature_extractor)\n",
    "\n",
    "batch_size= 1\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs = train_set[0]\n",
    "encoded_inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"].shape\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ...,   0,   0,   0],\n",
       "        [  0,   0,   0,  ...,   0,   0,   0],\n",
       "        [  0,   0,   0,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [252, 252, 252,  ..., 252, 252,   0],\n",
       "        [252, 252, 252,  ..., 252, 252, 252],\n",
       "        [252, 252, 252,  ..., 252, 252, 252]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForSemanticSegmentation were not initialized from the model checkpoint at microsoft/beit-base-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- beit.encoder.layer.0.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.0.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.1.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.1.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.2.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.2.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.3.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.3.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.4.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.4.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.5.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.5.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.6.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.6.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.7.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.7.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.8.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.8.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.9.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.9.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.10.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.10.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- beit.encoder.layer.11.attention.attention.relative_position_bias.relative_position_bias_table: found shape torch.Size([6244, 12]) in the checkpoint and torch.Size([732, 12]) in the model instantiated\n",
      "- beit.encoder.layer.11.attention.attention.relative_position_bias.relative_position_index: found shape torch.Size([1601, 1601]) in the checkpoint and torch.Size([197, 197]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([11, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- auxiliary_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([11, 256, 1, 1]) in the model instantiated\n",
      "- auxiliary_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeitForSemanticSegmentation(\n",
      "  (beit): BeitModel(\n",
      "    (embeddings): BeitEmbeddings(\n",
      "      (patch_embeddings): BeitPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): BeitEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): Identity()\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.036363635212183)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (6): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (7): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (8): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (9): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (10): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (11): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): Identity()\n",
      "  )\n",
      "  (fpn1): Sequential(\n",
      "    (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate=none)\n",
      "    (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (fpn2): Sequential(\n",
      "    (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (fpn3): Identity()\n",
      "  (fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (decode_head): BeitUperHead(\n",
      "    (classifier): Conv2d(768, 11, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (psp_modules): BeitPyramidPoolingModule(\n",
      "      (0): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=2)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=3)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (3): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=6)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bottleneck): BeitConvModule(\n",
      "      (conv): Conv2d(3840, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (lateral_convs): ModuleList(\n",
      "      (0): BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (fpn_convs): ModuleList(\n",
      "      (0): BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (fpn_bottleneck): BeitConvModule(\n",
      "      (conv): Conv2d(3072, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (auxiliary_head): BeitFCNHead(\n",
      "    (convs): Sequential(\n",
      "      (0): BeitConvModule(\n",
      "        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (classifier): Conv2d(256, 11, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"microsoft/beit-base-finetuned-ade-640-640\"\n",
    "filename = \"id2label.json\"\n",
    "id2label = json.load(open(\"id2label.json\", \"r\"))\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "config = BeitConfig.from_pretrained(model_name)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "config.num_labels = 11\n",
    "config.image_size = 224\n",
    "config.patch_size = 16\n",
    "model = BeitForSemanticSegmentation.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "#print(feature_extractor)\n",
    "print(model)\n",
    "#print(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a5d5c563e34e12afb7c0a6cab10576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 768, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22848\\3070781405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1263\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[0mauxiliary_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, encoder_hidden_states)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \u001b[0mlaterals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlateral_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlateral_conv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlateral_convs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m         \u001b[0mlaterals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpsp_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;31m# build top-down path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py\u001b[0m in \u001b[0;36mpsp_forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[0mpsp_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1055\u001b[1;33m         \u001b[0mpsp_outs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpsp_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m         \u001b[0mpsp_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsp_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsp_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[0mppm_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mppm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m             \u001b[0mppm_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m             upsampled_ppm_out = nn.functional.interpolate(\n\u001b[0;32m   1000\u001b[0m                 \u001b[0mppm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bilinear\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m             \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\transformers\\models\\beit\\modeling_beit.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    944\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \"\"\"\n\u001b[1;32m--> 168\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2434\u001b[0m         )\n\u001b[0;32m   2435\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2436\u001b[1;33m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2438\u001b[0m     return torch.batch_norm(\n",
      "\u001b[1;32mc:\\Users\\kaika\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2402\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2403\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2404\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expected more than 1 value per channel when training, got input size {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 768, 1, 1])"
     ]
    }
   ],
   "source": [
    "max_lr = 1e-3\n",
    "epochs = 15\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
    "                                            steps_per_epoch=len(train_loader))\n",
    "torch.cuda.empty_cache()\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "val_iou = []; val_acc = []\n",
    "train_iou = []; train_acc = []\n",
    "lrs = []\n",
    "min_loss = np.inf\n",
    "decrease = 1 ; not_improve=0\n",
    "\n",
    "model.to(device)\n",
    "fit_time = time.time()\n",
    "for e in range(epochs):\n",
    "    since = time.time()\n",
    "    running_loss = 0\n",
    "    iou_score = 0\n",
    "    accuracy = 0\n",
    "    #training loop\n",
    "    model.train()\n",
    "    for idx, data in enumerate(tqdm(train_loader)):\n",
    "        #training phase\n",
    "        pixel_values = data[\"pixel_values\"].to(device)\n",
    "        labels = data[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        output = model(pixel_values, labels=labels)\n",
    "        loss, logits = output.loss, output.logits\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step() #update weight          \n",
    "        optimizer.zero_grad() #reset gradient\n",
    "        \n",
    "        #step the learning rate\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        sched.step() \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            predicted = upsampled_logits.argmax(dim=1)\n",
    "            \n",
    "            # note that the metric expects predictions + labels as numpy arrays\n",
    "            metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "\n",
    "            # let's print loss and metrics every 100 batches\n",
    "        if idx % 100 == 0:\n",
    "            metrics = metric._compute(num_labels=len(id2label), \n",
    "                                    ignore_index=255,\n",
    "                                    reduce_labels=False, # we've already reduced the labels before)\n",
    "            )\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n",
    "                                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "191ae106892db87c2352df7ed3a4f90bfb00a0e75205aa92a952f04d1f482102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
