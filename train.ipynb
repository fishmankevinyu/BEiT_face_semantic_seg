{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitFeatureExtractor, BeitForSemanticSegmentation, BeitConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "from pathlib import Path, PurePosixPath, PureWindowsPath\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# load image + ground truth map\n",
    "\n",
    "path = '/Users/murlocy/EECE570/BEiT/headsegmentation_dataset_ccncsa/training.xml'\n",
    "image_folder = '/Users/murlocy/EECE570/BEiT/headsegmentation_dataset_ccncsa'\n",
    "\n",
    "def create_dataset(path, image_folder):\n",
    "    mytree = ET.parse(path)\n",
    "    myroot = mytree.getroot()\n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "    data_size = 100\n",
    "    for child in myroot:\n",
    "        try:\n",
    "            file_name = PureWindowsPath(child.attrib[\"name\"])\n",
    "            file_name = PurePosixPath(file_name)\n",
    "            image_path = image_folder /file_name\n",
    "            image = cv2.imread(str(image_path), cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            image = np.array(image)\n",
    "            seg_map = np.zeros((IMG_HEIGHT, IMG_WIDTH))\n",
    "            if child.tag == 'srcimg':\n",
    "                img_data_array.append(image)\n",
    "            elif child.tag == 'labelimg':    \n",
    "                for i in range(IMG_HEIGHT):\n",
    "                    for j in range(IMG_WIDTH):\n",
    "                        if np.array_equal(image[i,j],[0,0,0]):\n",
    "                            seg_map[i,j] = 0\n",
    "                        elif np.array_equal(image[i,j],[255,0,0]):\n",
    "                            seg_map[i,j] = 1\n",
    "                        elif np.array_equal(image[i,j],[0,255,0]):\n",
    "                            seg_map[i,j] = 2\n",
    "                        elif np.array_equal(image[i,j],[0,0,255]):\n",
    "                            seg_map[i,j] = 3\n",
    "                        elif np.array_equal(image[i,j],[128,128,128]):\n",
    "                            seg_map[i,j] = 4\n",
    "                        elif np.array_equal(image[i,j],[255,255,0]):\n",
    "                            seg_map[i,j] = 5\n",
    "                        elif np.array_equal(image[i,j],[255,0,255]):\n",
    "                            seg_map[i,j] = 6\n",
    "                        elif np.array_equal(image[i,j],[0,255,255]):\n",
    "                            seg_map[i,j] = 7\n",
    "                        elif np.array_equal(image[i,j],[255,255,255]):\n",
    "                            seg_map[i,j] = 8\n",
    "                        elif np.array_equal(image[i,j],[255,192,192]):\n",
    "                            seg_map[i,j] = 9\n",
    "                        elif np.array_equal(image[i,j],[0,128,128]):\n",
    "                            seg_map[i,j] = 10                \n",
    "                class_name.append(seg_map)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if len(class_name) == data_size:\n",
    "            break\n",
    "            \n",
    "    return img_data_array, class_name\n",
    "\n",
    "X, y = create_dataset(path, image_folder)\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [4. 4. 4. ... 0. 0. 0.]\n",
      " [4. 4. 4. ... 0. 0. 0.]\n",
      " [4. 4. 4. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y[0], interpolation='nearest')\n",
    "print(y[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, feature_extractor):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        encoded_inputs = self.feature_extractor(self.images[idx], self.labels[idx], return_tensors=\"pt\")\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_set = CustomDataset(X_train, y_train, feature_extractor)\n",
    "val_set = CustomDataset(X_test, y_test, feature_extractor)\n",
    "\n",
    "batch_size= 2\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs = train_set[0]\n",
    "encoded_inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([640, 640])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"].shape\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 4, 4, 4],\n",
       "        [0, 0, 0,  ..., 4, 4, 4],\n",
       "        [0, 0, 0,  ..., 4, 4, 4]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/db/50jrvdrj4v31yff8vfk92nbc0000gn/T/ipykernel_45097/827884743.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"mean_iou\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c0247f5eca471dabf7239d0cae2eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BeitForSemanticSegmentation were not initialized from the model checkpoint at microsoft/beit-base-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([11, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- auxiliary_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([11, 256, 1, 1]) in the model instantiated\n",
      "- auxiliary_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeitForSemanticSegmentation(\n",
      "  (beit): BeitModel(\n",
      "    (embeddings): BeitEmbeddings(\n",
      "      (patch_embeddings): BeitPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): BeitEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): Identity()\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.036363635212183)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (6): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (7): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (8): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (9): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (10): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (11): BeitLayer(\n",
      "          (attention): BeitAttention(\n",
      "            (attention): BeitSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (relative_position_bias): BeitRelativePositionBias()\n",
      "            )\n",
      "            (output): BeitSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BeitIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BeitOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): Identity()\n",
      "  )\n",
      "  (fpn1): Sequential(\n",
      "    (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (fpn2): Sequential(\n",
      "    (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (fpn3): Identity()\n",
      "  (fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (decode_head): BeitUperHead(\n",
      "    (classifier): Conv2d(768, 11, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (psp_modules): BeitPyramidPoolingModule(\n",
      "      (0): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=2)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=3)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (3): BeitPyramidPoolingBlock(\n",
      "        (0): AdaptiveAvgPool2d(output_size=6)\n",
      "        (1): BeitConvModule(\n",
      "          (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bottleneck): BeitConvModule(\n",
      "      (conv): Conv2d(3840, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "    (lateral_convs): ModuleList(\n",
      "      (0-2): 3 x BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (fpn_convs): ModuleList(\n",
      "      (0-2): 3 x BeitConvModule(\n",
      "        (conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (fpn_bottleneck): BeitConvModule(\n",
      "      (conv): Conv2d(3072, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (auxiliary_head): BeitFCNHead(\n",
      "    (convs): Sequential(\n",
      "      (0): BeitConvModule(\n",
      "        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (classifier): Conv2d(256, 11, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"microsoft/beit-base-finetuned-ade-640-640\"\n",
    "filename = \"id2label.json\"\n",
    "id2label = json.load(open(\"/Users/murlocy/EECE570/BEiT/id2label.json\", \"r\"))\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "feature_extractor = BeitFeatureExtractor.from_pretrained(model_name)\n",
    "config = BeitConfig.from_pretrained(model_name)\n",
    "config.id2label = id2label\n",
    "config.num_labels = 11\n",
    "model = BeitForSemanticSegmentation.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "#print(feature_extractor)\n",
    "print(model)\n",
    "#print(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd600f090f5e409c89267dec9f492665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_lr = 1e-3\n",
    "epochs = 15\n",
    "weight_decay = 1e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
    "                                            steps_per_epoch=len(train_loader))\n",
    "torch.cuda.empty_cache()\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "val_iou = []; val_acc = []\n",
    "train_iou = []; train_acc = []\n",
    "lrs = []\n",
    "min_loss = np.inf\n",
    "decrease = 1 ; not_improve=0\n",
    "\n",
    "model.to(device)\n",
    "fit_time = time.time()\n",
    "for e in range(epochs):\n",
    "    since = time.time()\n",
    "    running_loss = 0\n",
    "    iou_score = 0\n",
    "    accuracy = 0\n",
    "    #training loop\n",
    "    model.train()\n",
    "    for idx, data in enumerate(tqdm(train_loader)):\n",
    "        #training phase\n",
    "        pixel_values = data[\"pixel_values\"].to(device)\n",
    "        labels = data[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        output = model(pixel_values, labels=labels)\n",
    "        loss, logits = output.loss, output.logits\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step() #update weight          \n",
    "        optimizer.zero_grad() #reset gradient\n",
    "        \n",
    "        #step the learning rate\n",
    "        lrs.append(get_lr(optimizer))\n",
    "        sched.step() \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            predicted = upsampled_logits.argmax(dim=1)\n",
    "            \n",
    "            # note that the metric expects predictions + labels as numpy arrays\n",
    "            metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "\n",
    "            # let's print loss and metrics every 100 batches\n",
    "        if idx % 100 == 0:\n",
    "            metrics = metric._compute(num_labels=len(id2label), \n",
    "                                    ignore_index=255,\n",
    "                                    reduce_labels=False, # we've already reduced the labels before)\n",
    "            )\n",
    "\n",
    "            print(\"Loss:\", loss.item())\n",
    "            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n",
    "                                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
